{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic text classification ex",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDnNCylEl-0s",
        "outputId": "09669825-cbc5-4bb2-d48e-045fc761083b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "#binary sentiment analysis classifier\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract dataset from an online sourse\n",
        "url = \"http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"stack_overflow\", url, untar = True, cache_dir = '.', cache_subdir = '')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'stack_overflow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUGQDHCUmke7",
        "outputId": "e21222b3-38c1-4612-8b05-180b626c32dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
            "6053888/6053168 [==============================] - 0s 0us/step\n",
            "6062080/6053168 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset_dir)\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "CSEk4iSLno0q",
        "outputId": "f25038fa-49fc-4a2c-e32e-d11d2a8a61e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1878810d94e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './stack_overflow'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/1181_9.txt') \n",
        "with open(sample_file) as f: \n",
        "  print(f.read())"
      ],
      "metadata": {
        "id": "oGDMX-bjotBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove additional folders\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "5QJtRYPXIJZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#divide dataset into train, cval, and test\n",
        "\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'stack_overflow/train', \n",
        "    batch_size = batch_size,\n",
        "    validation_split = 0.2, #use 80% of the examples in the training folder for training\n",
        "    subset = 'training', \n",
        "    seed = seed\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ew3d0VUPIdtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])"
      ],
      "metadata": {
        "id": "D3WDTfdFL-ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'stack_overflow/train', \n",
        "    batch_size = batch_size,\n",
        "    validation_split = 0.2, \n",
        "    subset = 'validation', \n",
        "    seed = seed\n",
        ")"
      ],
      "metadata": {
        "id": "ZwE2dkNcMtAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'stack_overflow/test', \n",
        "    batch_size = batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "x43eRoNQOPzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardize, tokenize, and vectorize data\n",
        "#All accomplished using tf.keras.layers.TextVectorization layer!\n",
        "#However, this won't remove HTML tags, so you need to build a custom fn. \n",
        "\n",
        "def custom_standardization(input_data): \n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')"
      ],
      "metadata": {
        "id": "xYVY5zxMOiJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 1000\n",
        "sequence_length = 250 #explicit maximum token length\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize = custom_standardization, #calling above fn \n",
        "    max_tokens = max_features,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length\n",
        ")\n"
      ],
      "metadata": {
        "id": "wqBvMCDOPgFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "#gets the model to build an index of strings to integers\n",
        "vectorize_layer.adapt(train_text)\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "metadata": {
        "id": "FPpkaepOQUa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization of the first example\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "metadata": {
        "id": "GxnSQjjCRVC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the TextVectorization layer to the train, cval, and test datasets\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "8-cyOiQFPbrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cache the data for better performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size = AUTOTUNE)"
      ],
      "metadata": {
        "id": "68Tm6ZlAQbmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim), \n",
        "  layers.Dropout(0.2), #fraction of the input units to drop\n",
        "  layers.GlobalAveragePooling1D(), \n",
        "  layers.Dropout(0.2), \n",
        "  layers.Dense(4)  \n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "0XV8tS8qZCoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits = True), optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "ZHGep91piPNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model \n",
        "epochs = 10\n",
        "history = model.fit(train_ds, validation_data = val_ds, epochs = epochs)"
      ],
      "metadata": {
        "id": "AmHcGUOyikck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "2I0ewQLyi6h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a plot of accuracy and loss over time\n",
        "# model.fit() returns a History object that contains a dictionary with everything\n",
        "# that happened during training\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "id": "v5eXnZKijNl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "metadata": {
        "id": "BNCFgG_hjlCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Validation accuracy plot\n",
        "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kNZ-O0STk4uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "    vectorize_layer, \n",
        "    model, \n",
        "    layers.Activation('sigmoid')                                \n",
        "])\n",
        "export_model.compile(\n",
        "    loss = losses.BinaryCrossentropy(from_logits = False), optimizer = \"adam\", metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "BvPd_xUmkQ-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on new data\n",
        "examples = [\n",
        "  \"The movie was great!\", \n",
        "  \"The movie was okay.\",\n",
        "  \"The movie was terrible...\"\n",
        "]\n",
        "predictions_ = export_model.predict(examples)\n",
        "print(predictions_)\n"
      ],
      "metadata": {
        "id": "wDLTfL-4mkdL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}